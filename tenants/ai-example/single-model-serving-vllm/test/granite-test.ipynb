{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8afde-9b18-4b6a-9ee5-33924bdb4f16",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# REST Inference\n",
    "# Used from: https://github.com/cfchase/basic-kserve-vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c004acc-13cd-4917-8480-592c7c2d623b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Change that following variable settings match your deployed model's *Inference endpoint*. for example: \n",
    "\n",
    "```\n",
    "infer_endpoint = \"https://model-vllm.apps.clusterx.sandboxx.opentlc.com\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de65d02-84a6-4cff-882e-551cdd42b486",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_endpoint = \"https://granite-ai-example-single-model-serving.apps.cluster-97p5w.97p5w.sandbox659.opentlc.com\"\n",
    "infer_url = f\"{infer_endpoint}/v1/completions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f9ece-e9cf-44e2-a8a2-73160186aee8",
   "metadata": {},
   "source": [
    "## Request Function\n",
    "\n",
    "Build and submit the REST request. \n",
    "\n",
    "Note: You submit the data in the same format that you used for an ONNX inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9386f-683a-4880-b780-c40bec3ab9f8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def rest_request(prompt):\n",
    "    json_data = {\n",
    "        \"model\": \"/mnt/models/\",\n",
    "        \"prompt\": [\n",
    "            prompt\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"logprobs\": 0,\n",
    "        \"echo\": False,\n",
    "        \"stop\": [\n",
    "            \"string\"\n",
    "        ],\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"best_of\": 1,\n",
    "        \"user\": \"string\",\n",
    "        \"top_k\": -1,\n",
    "        \"ignore_eos\": False,\n",
    "        \"use_beam_search\": False,\n",
    "        \"stop_token_ids\": [\n",
    "            0\n",
    "        ],\n",
    "        \"skip_special_tokens\": True,\n",
    "        \"spaces_between_special_tokens\": True,\n",
    "        \"repetition_penalty\": 1,\n",
    "        \"min_p\": 0,\n",
    "        \"include_stop_str_in_output\": False,\n",
    "        \"length_penalty\": 1\n",
    "    }\n",
    "\n",
    "    # If using RHOAI 2.13 or new, set `verify=True`\n",
    "    # Older versions utilize a self-signed cert that is not trusted by default\n",
    "    response = requests.post(infer_url, json=json_data, verify=True)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad16ac-23da-48bd-9796-f8e4cacae981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = rest_request(\"What is AI?\")\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
